# Deepfake Detector Assessment Platform (DAP)

We have set up a project for evaluating deepfake detection. Users can upload detection algorithms, and the project will assess the performance of these algorithms in various aspects.

The main evaluation features of the project are shown in the following figure.

![image1](https://github.com/tempuser4567/DAP/tree/main/png/1.png)
![image0](https://github.com/tempuser4567/DAP/tree/main/png/1.png)


## Dataset Introduction:

Our dataset consists of two parts: public datasets and self-constructed datasets. The public datasets include 9 open-source datasets. The self-constructed dataset is generated using deepfake algorithms and includes 3 face-swapping algorithms, 2 expression-driven algorithms, 2 attribute-editing algorithms, 3 full-face synthesis algorithms, 3 text-to-image algorithms, and 1 image-to-video algorithm, totaling 14 deepfake algorithms.



These public datasets include data from different deepfake algorithms, genders, ethnicities, expressions, lighting conditions, and camera angles, which can be used to evaluate the performance of detection algorithms in these specific areas. 

The self-constructed dataset includes data generated by diverse deepfake algorithms, which can be used to assess the generalization capability of detection algorithms.

![image2](https://github.com/tempuser4567/DAP/tree/main/png/2.png)

We have implemented several data augmentation algorithms, such as adjusting image brightness, contrast, hue, sharpness, rotation, blurring, and occlusion, to evaluate the robustness of detection algorithms. 

Additionally, we have configured adversarial (adding adversarial noise) and evasion (image reconstruction) algorithms to assess the security of detection algorithms when faced with malicious adversarial and evasion techniques.

![image3](https://github.com/tempuser4567/DAP/tree/main/png/3.png)

### Data Composition: 

Our data consists of three levels: video-level, frame-level, and face-level. The latter two are obtained by extracting frames and faces from the original videos, respectively, and are used to evaluate detection algorithms that target different inputs. The overall data structure is as follows:

```text
datasets
├── FaceForensics++
│ ├── original_sequences
│ │ ├── youtube
│ │ │ ├── c23
│ │ │ │ ├── videos
│ │ │ │ │ └── *.mp4
│ │ │ │ └── frames
│ │ │ │ │ └── *.png
│ │ │ │ └── faces
│ │ │ │ │ └── *.png
│ │ │ └── c40
│ │ │ │ ├── ...
│ │ ├── actors
│ │ │ ├── ...
│ ├── manipulated_sequences
│ │ ├── Deepfakes
│ │ │ ├── c23
│ │ │ │ └── videos
│ │ │ │ │ └── *.mp4
│ │ │ │ └── frames
│ │ │ │ │ └── *.png
│ │ │ └── c40
│ │ │ │ ├── ...
│ │ ├── Face2Face
│ │ │ ├── ...
│ │ ├── FaceSwap
│ │ │ ├── ...
│ │ ├── NeuralTextures
│ │ │ ├── ...
│ │ ├── FaceShifter
│ │ │ ├── ...
│ │ └── DeepFakeDetection
│ │ ├── ...
```
Other datasets are similar to the above structure



Our labeling structure is as follows:

```
datasets
├── FaceForensics++
│ ├── videos
│ │ ├── attribute
│ │ │ └── FaceForensics++_fake_DeepFakeDetection_c23_videos.txt
│ │ │ └── FaceForensics++_real_actors_c23_videos.txt
│ │ │ └── ...
│ │ └── FaceForensics++_real_videos.txt
│ │ └── FaceForensics++_fake_videos.txt
│ ├── frames
│ │ ├── ...
│ ├── faces
│ │ ├── ...
```

The attributes folder contains data labels that provide detailed categorization based on specific attributes such as forgery methods, ethnicity, gender, and other characteristics.


### Test Annotations:

For evaluating detection algorithms, we select 10,000 real images and 10,000 forged images from the corresponding datasets for result testing.

The specific test items and the corresponding datasets are shown in the table below:

| Test Content                   | Dataset                                   |
| ------------------------------ | ----------------------------------------- |
| Effectiveness Testing          | All public datasets                       |
| Specific Attribute Testing     | Datasets with corresponding label files   |
| Generalization Testing         | All self-constructed datasets             |
| Robustness Testing             | Robustness processed dataset              |
| Security Testing               | Adversarial and evasion processed dataset |
| Specific Functionality Testing | Datasets with corresponding label files   |

The test content and metrics are as follows:

Effectiveness Testing: The test evaluates the basic performance metrics of detection algorithms on a general dataset, including AUC, ACC, EER, F1/F2-score, confidence, and other indicators.

Specific Attribute Testing: The test assesses the detection capability on datasets with specific attributes.   (??? test metrics)

Generalization Testing: The test evaluates the generalization capability of detection algorithms.

Robustness Testing: The test assesses the robustness of detection algorithms when faced with traditional data modifications and enhancements.

Security Testing: The test evaluates the detection capability of algorithms when facing malicious evasion and attack forgery data.

Specific Functionality Testing: The test evaluates the special functionalities of detection algorithms, if they exist, such as forged region localization, fragment forgery detection, etc.



## Algorithms Introduction

We have configured some forgery algorithms and detection algorithms. Forgery algorithms are used to generate data (see the introduction in the self-constructed dataset section), while detection algorithms are used to test metrics and provide baselines for existing detection algorithms. Currently, we have configured 10 forgery algorithms, listed as follows.

| ID   | name               | function                             |
| ---- | ------------------ | ------------------------------------ |
| 1    | Xception           | Detection of forged face-level data  |
| 2    | SRM                | Detection of forged face-level data  |
| 3    | SBI                | Detection of forged face-level data  |
| 4    | DSP_FWA            | Detection of forged face-level data  |
| 5    | Multiple-attention | Detection of forged face-level data  |
| 6    | SeqDeepFake        | Detection of forged face-level data  |
| 7    | SLADD              | Detection of forged face-level data  |
| 8    | CADDM              | Detection of forged frame-level data |
| 9    | Multiple-attention | Detection of forged video-level data |
| 10   | ClassNSeg          | Detecting forged regions             |
| 11   | BA-TFD             | Detecting forged segments            |

## Presentation of Results
